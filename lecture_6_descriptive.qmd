---
title: "Descriptive statistics and Introduction to statistical inference"
subtitle: "Bio300B Lecture 6"
author: "Richard J. Telford (Richard.Telford@uib.no)"
institute: "Institutt for biovitenskap, UiB"
date: today
date-format: "D MMMM YYYY"
format: 
  revealjs:
    theme: [default, css/custom.scss]
    resources: 
      - shinylive-sw.js
filters:
  - shinylive
execute: 
  echo: false
  warning: false
  error: true
---

## Describing a distribution

```{r setup, echo=FALSE, message=FALSE}
library("tidyverse")
library("gganimate")
data(penguins, package = "palmerpenguins")
theme_set(theme_bw(base_size = 18))
library(patchwork)
```

```{r a-dist}
p_dist <- penguins |>
  filter(species == "Gentoo") |>
  ggplot(aes(x = bill_length_mm)) +
  geom_histogram(bins = 25, fill = "grey50") +
  labs(x = "Bill length mm", y = "Count") +
  scale_colour_brewer(palette = "Set1")
p_dist
```

## Midpoint

```{r midpoint, fig.width = 9, fig.height = 5}
mid <- penguins |>
  filter(species == "Gentoo") |>
  drop_na() |>
  summarise(
    mean = mean(bill_length_mm),
    median = median(bill_length_mm),
    .groups = "drop"
  ) |>
  pivot_longer(everything())

p_dist +
  geom_segment(aes(x = value, xend = value, y = 5, yend = 0, colour = name),
    data = mid, arrow = arrow(type = "closed")
  ) +
  theme(
    legend.position = c(0.99, 0.99), legend.justification = c(1, 1),
    legend.title = element_blank()
  )
```

## Mean

::: columns
::: {.column width="50%"}
```{r mean, fig.width = 4.5, fig.height = 4.5}
p_dist +
  geom_segment(aes(x = value, xend = value, y = 5, yend = 0, colour = name),
    data = mid |> filter(name == "mean"), arrow = arrow(type = "closed")
  ) +
  theme(
    legend.position = c(0.99, 0.99), legend.justification = c(1, 1),
    legend.title = element_blank()
  )
```
:::

::: {.column width="50%"}
$$\overline{x} = \frac{\sum_{i=1}^n x_i}{n}$$

-   Sum of all observations divided by number of observations
-   Centre of gravity of the data
-   `mean()`
:::
:::

## Median

```{r median-anim, cache = TRUE}
median_setup <- penguins |>
  filter(species == "Gentoo") |>
  drop_na(bill_length_mm) |>
  select(bill_length_mm) |>
  mutate(n = seq_len(n()), order = n, fill = "grey50")

median_anim <- bind_rows(
  a_raw = median_setup,
  b_sorted = median_setup |>
    mutate(order = rank(bill_length_mm, ties.method = "first")),
  c_highlight = median_setup |>
    mutate(
      order = rank(bill_length_mm, ties.method = "first"),
      fill = if_else(
        between(order, (n() + 1) / 2 - 0.5, (n() + 1) / 2 + 0.5),
        true = "red", false = fill
      )
    ),
  .id = "what"
) |>
  mutate(order = order * 1.0) |>
  ggplot(aes(x = order, y = bill_length_mm, group = n, fill = I(fill))) +
  geom_col() +
  scale_x_continuous(expand = c(0.01, 0)) +
  labs(x = "Order", y = "Bill length mm") +
  transition_states(what, transition_length = c(1, 1, 1), state_length = c(1, .1, 1), wrap = FALSE) +
  ease_aes("cubic-in-out")

median_anim
```

-   Sort from minimum to maximum
-   Observation in the middle is the median
-   `median()`

## Mean vs Median

```{r biomass-import, include=FALSE}
biomass <- read_delim("data/biomass2015_H.csv", delim = ",") |>
  mutate(production = as.numeric(production))
```

```{r biomass-plot}
biomass_plot <- ggplot(biomass, aes(x = production)) +
  geom_histogram(boundary = 0) +
  labs(x = "Biomass, g", title = "Mt Gonga Biomass data")
biomass_plot
```

## Mean vs Median

```{r biomass-plot2}
mid_biomass <- biomass |>
  drop_na(production) |>
  summarise(
    mean = mean(production),
    median = median(production),
    .groups = "drop"
  ) |>
  pivot_longer(everything())

biomass_plot +
  geom_segment(aes(x = value, xend = value, y = 15, yend = 0, colour = name), data = mid_biomass, arrow = arrow(type = "closed")) +
  labs(y = "Count") +
  theme(
    legend.position = c(0.99, 0.99),
    legend.justification = c(1, 1),
    legend.title = element_blank()
  )
```

## Calculating Mean and Median in R

```{r mean-median, echo=TRUE}
vec <- c(56.7, 57.2, 62.7, 56.4, 60.8)
# mean of a vector
mean(vec)
# median of a vector
median(vec)
```

------------------------------------------------------------------------

Use `na.rm = TRUE` to remove missing values.

```{r mean-median-NA, echo=TRUE}
vec <- c(56.7, 57.2, 62.7, 56.4, 60.8, NA)
# mean of a vector with NA
mean(vec)
# mean of a vector excluding NA
mean(vec, na.rm = TRUE)
```

------------------------------------------------------------------------

```{r mean-median2, echo=TRUE}
# median of a column
median(penguins$bill_length_mm, na.rm = TRUE)
# or
penguins |>
  summarise(med = median(bill_length_mm, na.rm = TRUE))

# mean of a column with a grouping variable
penguins |>
  group_by(species) |>
  summarise(
    med = median(bill_length_mm, na.rm = TRUE),
    .groups = "drop"
  )
```

## Spread

```{r spread, fig.width = 9, fig.height = 5}
max_min <- penguins |>
  filter(species == "Gentoo") |>
  drop_na() |>
  summarise(
    minimum = min(bill_length_mm),
    maximum = max(bill_length_mm)
  )

mean_sd <- penguins |>
  filter(species == "Gentoo") |>
  drop_na() |>
  summarise(
    mean = mean(bill_length_mm),
    mean_sd = mean(bill_length_mm) + sd(bill_length_mm)
  )
p_dist +
  geom_segment(
    aes(x = value, xend = value, y = 5, yend = 0, colour = name),
    data = max_min |>
      pivot_longer(everything()),
    arrow = arrow(type = "closed")
  ) +
  geom_segment(
    aes(x = minimum, xend = maximum, y = 7, yend = 7, colour = "Range"),
    data = max_min,
    arrow = arrow(type = "closed", ends = "both")
  ) +
  geom_segment(
    aes(x = mean, xend = mean_sd, y = 10, yend = 10, colour = "Standard Deviation"),
    data = mean_sd,
    arrow = arrow(type = "closed", ends = "last")
  ) +
  theme(
    legend.position = c(0.99, 0.99), legend.justification = c(1, 1),
    legend.title = element_blank()
  )
```

## Minimum, maximum and range

-   `min()`
-   `max()`

Range is difference between smallest and largest.

```{r max-min-range, echo = TRUE}
# range() gives smallest and largest values
range(penguins$bill_length_mm, na.rm = TRUE)

# calculate range as
max(penguins$bill_length_mm, na.rm = TRUE) -
  min(penguins$bill_length_mm, na.rm = TRUE)

# or
diff(range(penguins$bill_length_mm, na.rm = TRUE))
```

## Variance

The average squared distance around the mean `var()`

### Population variance

$$\sigma^2 = \frac{\sum_{i=1}^n (x_i - \mu)^2}{n}$$ Where $\mu$ is the population mean.

### Sample variance

$$s^2 = \frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n-1}$$ Where $\overline{x}$ is the sample mean.\



## Standard deviation

::: columns
::: {.column width="50%"}
Square root of the variance $$s = \sqrt{s^2}$$ $$s  = \sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n-1}}$$ Same units as variable

`sqrt(var())`\
`sd()`
:::

::: {.column width="50%"}
```{r sd, fig.height = 4, fig.width = 4}
bind_rows(
  `5` = tibble(x = 10:70, y = dnorm(x, mean = 40, sd = 5)),
  `10` = tibble(x = 10:70, y = dnorm(x, mean = 40, sd = 10)),
  .id = "sd"
) |>
  ggplot(aes(x = x, y = y, fill = sd)) +
  geom_area(alpha = 0.5, position = "identity") +
  scale_fill_brewer(palette = "Set1") +
  labs(x = "Value", y = "Density") +
  theme(
    legend.position = c(0.99, 0.99),
    legend.justification = c(1, 1)
  )
```
:::
:::

------------------------------------------------------------------------

```{r}
tibble(x = 5:75, y = dnorm(x, mean = 40, sd = 10), sd = 10) |>
  ggplot(aes(x = x, y = y, fill = factor(sd))) +
  geom_area(alpha = 0.5, position = "identity", show.legend = FALSE) +
  annotate(geom = "segment", x = 40, xend = 40, y = 0, yend = dnorm(40, mean = 40, sd = 10), linetype = "dashed") +
  annotate(geom = "text", x = 40, y = dnorm(40, mean = 40, sd = 10), label = "mean", hjust = 0) +
  annotate(geom = "segment", x = 30, xend = 50, y = dnorm(30, mean = 40, sd = 10), yend = dnorm(50, mean = 40, sd = 10)) +
  annotate(geom = "text", x = 50, y = dnorm(30, mean = 40, sd = 10), label = "mean %+-% 1 ~ SD", hjust = 0, parse = TRUE) +
  annotate(geom = "segment", x = 20, xend = 60, y = dnorm(20, mean = 40, sd = 10), yend = dnorm(60, mean = 40, sd = 10)) +
  annotate(geom = "text", x = 60, y = dnorm(60, mean = 40, sd = 10), label = "mean %+-% 2 ~ SD", hjust = 0, parse = TRUE) +
  scale_fill_brewer(palette = "Set1") +
  labs(x = "Value", y = "Density") +
  theme(
    legend.position = c(0.99, 0.99),
    legend.justification = c(1, 1)
  )
```

# Higher moments

## Skew

::: columns
::: {.column width="50%"}
```{r biomassplot-again, fig.height = 5, fig.width = 5}
biomass_plot
```
:::

::: {.column width="50%"}
Right skew\
positive skew `e1071::skewness()`
:::
:::

## Kurtosis

```{r kurtosis, fig.height = 5, fig.width = 5}
crossing(
  x = seq(-7, 7, length = 100),
  df = c(1)
) |>
  mutate(y = dt(x, df = df)) |>
  ggplot(aes(x = x, y = y)) +
  geom_area(fill = "grey") +
  geom_line(data = tibble(x = seq(-7, 7, length = 100), y = dnorm(x, sd = 1)), colour = "black") +
  labs(colour = "df", y = "Density")
```

heavy tails

`e1071::kurtosis()`

## Making inferences about the population

Sample: `r penguins |> filter(species == "Gentoo") |> drop_na(bill_length_mm) |> nrow()` Gentoo penguins

Population: All Gentoo penguins in the Palmer Archipelago.

What can we infer about the population from the sample?

## Uncertainty in the mean

```{r mean-uncertainty}
dat <- tibble(x = rnorm(1e6, mean = 100, sd = 10))

p1 <- ggplot(dat, aes(x = x)) +
  geom_density(fill = "grey80") +
  geom_vline(xintercept = 100, colour = "red", linetype = "dashed")

dat2 <- bind_rows(
  small = map(1:1000, ~ slice_sample(dat, n = 20)) |> list_rbind(names_to = "run"),
  large = map(1:1000, ~ slice_sample(dat, n = 100)) |> list_rbind(names_to = "run"),
  .id = "size"
) |>
  mutate(run = as.numeric(run))

means <- dat2 |>
  group_by(run, size) |>
  summarise(mean = mean(x), .groups = "drop")

p2 <- ggplot(dat2 |> filter(run <= 10), aes(x = x)) +
  geom_histogram(bins = 20) +
  geom_segment(data = means |> filter(run <= 10), aes(x = mean, xend = mean, y = 5, yend = 0), arrow = arrow(type = "closed"), colour = "red") +
  facet_wrap(facets = vars(size)) +
  labs(y = "Count") +
  transition_states(states = run, transition_length = 1, state_length = 2) +
  ease_aes("cubic-in-out")

p3 <- ggplot(means, aes(x = mean)) +
  geom_histogram() +
  facet_wrap(facets = vars(size)) +
  geom_vline(xintercept = 100, colour = "red", linetype = "dashed")
```

```{r pop, fig.height = 5}
p1
```

Population of 1,000,000 indviduals

Large (100 individuals) or small (20 individuals) samples

------------------------------------------------------------------------

Histograms of samples

```{r samples, cache=TRUE, fig.width = 6, fig.height = 3}
animate(p2, renderer = gifski_renderer())
```

------------------------------------------------------------------------

Histogram of sample means

```{r sample-means, fig.width = 6, fig.height = 3}
p3
```

## Standard error of the mean

Standard deviation divided by square root of number of observations

$$SE = \frac{s}{\sqrt{n}}$$ Large n - small SE - reliable estimate of mean

Small n - large SE - less reliable estimate of mean

## Confidence interval

If experiment repeated many times, confidence level represents the proportion of confidence intervals that contain the true value of the unknown population parameter.

95% confidence interval will contain true value in 95% of repeat experiments

Easily misunderstood.

(Bayesian credibility intervals are much more intuitive)

## Confidence interval of the mean

95% confidence interval

Mean $\pm$ 1.96 \* SE

Why 1.96?

```{r se1.96, fig.height=4}
tibble(
  x = seq(-3, 3, length = 1000),
  y = dnorm(x, mean = 0, sd = 1),
  z = case_when(x < -1.96 ~ "lower", x > 1.96 ~ "upper", TRUE ~ "a")
) |>
  ggplot(aes(x = x, y = y, fill = z)) +
  geom_area(show.legend = FALSE) +
  scale_fill_manual(values = c(lower = scales::muted("red"), upper = scales::muted("red"), a = "grey")) +
  labs(x = "z", y = "") +
  scale_x_continuous(breaks = c(-1.96, 0, 1.96))
```

Grey area is 95% of the normal distribution

---

Estimated means with 95% confidence interval for different sample sizes

```{r}
dat2 |>
  filter(run < 50) |>
  ggplot(aes(x = run, y = x)) +
  geom_hline(yintercept = 100, colour = "red") +
  stat_summary(fun.data = "mean_se", fun.args = list(mult = 1.96)) +
  facet_wrap(facets = vars(size)) +
  coord_flip() +
  labs(y = "value")
```

## Choosing a statistical test

1)  What is the hypothesis?\
2)  What is the underlying distribution of your response variable?\
3)  What type of the predictor(s)?\
4)  What type of observational or experimental design do you have?

## The hypothesis

The statistical hypothesis needs to match your scientific hypothesis

Hypothesis example:\
H<sub>0</sub>: The bill length of Gentoo penguins does not depend on sex.

## The predictors

-   Categorical (sex, species)
-   Continuous (body mass)
-   both (with \> 1 predictor)

Need to know so we can code predictor variables and interpret the output of our models.

## The response

-   Continuous (bill length)
-   count (number of penguins)
-   binary (sex, survived/died)
-   proportion (8 out of 10)

Statistical model families (lm, lme, glm etc.) differ in assumptions about the underlying distribution of the response variable.

## Gaussian (Normal) distribution

Often assumed for continuous distributions, body mass, egg shell thickness

```{r, fig.height = 5}
p1
```

## Poisson distribution

Count data

Shape varies with mean

```{r poisson, fig.height = 5}
bind_rows(
  `mean = 1` = tibble(x = rpois(1000, lambda = 1)),
  `mean = 5` = tibble(x = rpois(1000, lambda = 5)),
  `mean = 10` = tibble(x = rpois(1000, lambda = 10)),
  .id = "lambda"
) |>
  mutate(lambda = factor(lambda, levels = c("mean = 1", "mean = 5", "mean = 10"))) |>
  ggplot(aes(x = x)) +
  geom_bar() +
  facet_wrap(facets = vars(lambda))
```

## Experimental design

-   Independent observations?
-   time series?
-   clustered design?

```{r tank, out.width="50%"}
knitr::include_graphics("figures/46695412244_66616e8523_c.jpg")
```

## Key

-   Independent observations
    -   Continuous response = linear models (`lm()`, `t.test()`)
    -   Count/binary/proportion = generalised linear models (`glm()`)
-   Clustered observations
    -   Continuous response = linear mixed effect models (`lme()`, `lmer()`)
    -   Count/binary/proportion = generalised linear mixed effect models (`glmer()`)

## A t-test

Compare means

H<sub>0</sub>: Bill length does not depend on sex in Gentoo penguins

```{r t-test-plot, fig.height = 4}
penguins |>
  filter(species == "Gentoo") |>
  drop_na(bill_length_mm, sex) |>
  ggplot(aes(x = bill_length_mm, fill = sex)) +
  geom_histogram(show.legend = FALSE) +
  scale_fill_viridis_d() +
  facet_wrap(facets = vars(sex), nrow = 1)
```

## t-test in R

```{r echo = TRUE}
gentoo <- penguins |>
  filter(species == "Gentoo") |>
  drop_na(bill_length_mm, sex)

mod1 <- t.test(bill_length_mm ~ sex, data = gentoo)
```

---

```{r}
mod1
```

## P values

Often misinterpreted

-   Not a measure of effect size or practical significance
-   Not the probability that hypothesis is true
-   Strongly affected by sample size

*If there were actually no effect (if the true difference between means were zero) then the probability of observing a value for the difference equal to, or greater than, that actually observed would be p=0.05.*

Many assumptions

Confidence intervals are more interpretable

## Type 1 and type 2 errors


```{r}
library(gt)
correct <- "#a0ffa0"
error <- "orange"
tibble(
  `Null hypothesis is` = c("Rejected", "Not rejected"),
  True = c("Type I error<br>False positive", "Correct decision<br>True negative"),
  False = c("Correct decision<br>True positive", "Type II error<br>False negative")
) |>
  gt::gt() |>
  tab_style(
    style = cell_fill(color = correct),
    locations = cells_body(
      columns = c(True),
      rows = str_detect(True, "Correct")
    )
  ) |>
  tab_style(
    style = cell_fill(color = correct),
    locations = cells_body(
      columns = c(False),
      rows = str_detect(False, "Correct")
    )
  ) |>
  tab_style(
    style = cell_fill(color = error),
    locations = cells_body(
      columns = c(True),
      rows = !str_detect(True, "Correct")
    )
  ) |>
  tab_style(
    style = cell_fill(color = error),
    locations = cells_body(
      columns = c(False),
      rows = !str_detect(False, "Correct")
    )
  ) |>
  fmt_markdown(columns = everything()) |>
  tab_options(table.font.size = 30)
```


## One sided Z-test

```{r TP-FP, echo = FALSE}
library(tidyverse)
n <- 10
alpha <- 0.05
delta <- 1
sd <- 1
mx <- 2.5
mn <- -1.5
crit <- qnorm(alpha, mean = 0, sd = sd / sqrt(n), lower.tail = FALSE)
H0 <- tibble(
  x = seq(mn, mx, length = 201),
  y = dnorm(x = x, mean = 0, sd = sd / sqrt(n)),
  what = if_else(x < crit, true = "True negative", false = "False positive")
)
H1 <- tibble(
  x = seq(mn, mx, length = 201),
  y = dnorm(x = x, mean = delta, sd = sd / sqrt(n)),
  what = if_else(x > crit, true = "True positive", false = "False negative")
)

H <- bind_rows(
  `H[0]~is~true` = H0,
  `H[1]~is~true` = H1,
  .id = "Hypothesis"
)
ggplot(H, mapping = aes(x = x, y = y, fill = what)) +
  geom_area() +
  geom_line(aes(group = 1)) +
  geom_vline(aes(xintercept = crit, linetype = "Critical Value")) +
  facet_wrap(facets = vars(Hypothesis), ncol = 1, labeller = label_parsed) +
  labs(x = "x", y = "Density", linetype = "", fill = "")
```


## 
{{< include includes/_one-sided-z-power.qmd >}}


## The need for power

With little power:

-   May not be able to reject H<sub>0</sub> when it is false
-   Exaggerate effect size

Lots of power

-   Probably can reject H<sub>0</sub> when it is false
-   More precise estimates of effect size
-   More expensive

Need to do power analysis before experiment.

## Components of a power analysis

-   Effect size
-   Type I error rate (significance level - conventionally set to 0.05)
-   Power (1 - Type II error rate) - conventionally aim for 0.8
-   Number of observations

Can solve for any of these

Typically want to know how many observations needed.

## Analytic power analysis

Some power tests in base R.

-   `power.t.test`
-   `power.anova.test`
-   `power.prop.test`

More in `pwr` package

## Power t test

```{r pwr, echo = TRUE}
library("pwr")
pow <- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8)
pow
```

Effect size is Cohen's d $d = \frac{\mu_1 - \mu_2}{\sigma}$

------------------------------------------------------------------------

```{r pwr-plot, echo = FALSE}
plot(pow)
```

------------------------------------------------------------------------

-   Power test should be done before experimental work to determine sample size
-   Analytical and simulation approaches are possible
-   Key challenge is estimating effect size
    -   existing estimates are likely biased
    -   minimum interesting effect size
