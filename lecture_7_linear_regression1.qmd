---
title: "Linear Regression 1"
subtitle: "Bio300B Lecture 7"
author: "Richard J. Telford (Richard.Telford@uib.no)"
institute: "Institutt for biovitenskap, UiB"
date: today
date-format: "D MMMM YYYY"
format: 
  revealjs:
    theme: [default, css/custom.scss]
    resources: 
      - shinylive-sw.js
filters:
  - shinylive
execute: 
  echo: true
  warning: false
  error: true
---


## Bivariate descriptive statistics

```{r setup, echo=FALSE, message=FALSE}
library("tidyverse")
library("patchwork")
library(broom)
library(glue)
library(conflicted)
conflict_prefer_all("dplyr")

theme_set(theme_bw(base_size = 18))
data(penguins, package = "palmerpenguins")
options(dplyr.print_min = 2, dplyr.print_max = 3, digits = 4)
```

:::: {.columns}

::: {.column width="50%"}
Measures of association

  - covariance
  - correlation
:::

::: {.column width="50%"}
Use with

- two continuous variables
- paired data
- unclear direction of causality
:::

::::

```{r corr, fig.width = 5, fig.height = 5, echo = FALSE}
gentoo <- penguins |>
  filter(species == "Gentoo") |>
  drop_na(bill_length_mm)
g1 <- ggplot(gentoo, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point() +
  labs(x = "Bill length mm", y = "Bill depth mm")
g1
```


## Covariance

:::: {.columns}

::: {.column width="50%"}
Association between two variables

$S_{xy} = \frac{\Sigma (x_i - \mu_x)(y_i - \mu_y)}{n - 1}$

$S_{xy} = S_{yx}$

- \-inf, 0, +inf
- \+ = positive association
- \- = negative association
- `cov()`
:::

::: {.column width="50%"}
![](`r knitr::fig_chunk('corr', 'png')`)
:::

::::

## Correlation

:::: {.columns}

::: {.column width="50%"}
Pearson coefficient of correlation

Standardised association

- $S_{xy}$ - covariance of x & y
- $S_x^2$ - variance of x
- $S_y^2$ - variance of y

$r_{xy} = \frac{S_{xy}}{\sqrt{S_x^2S_y^2}}$

$r_{xy} = r_{yx}$
:::

::: {.column width="50%"}
- \-1, 0, +1
- \+ = positive association
- \- = negative association

![](`r knitr::fig_chunk('corr', 'png')`)
:::

::::


## Correlations in R

```{r cor}
cor(
  x = penguins$bill_length_mm,
  y = penguins$body_mass_g,
  use = "pairwise.complete"
)
```

```{r cor-mat}
penguins |>
  select(bill_length_mm:body_mass_g) |>
  cor(use = "complete.obs")
```


## $R^2$

Coefficient of determination 

$$R^2 = r^2$$

- 0 - 1
- proportion of variance explained
- $R^2$ = 0.5, 50% of variation in data explained

## Testing a Correlation

```{r cor-test}
cor.test(penguins$bill_length_mm,
  penguins$body_mass_g,
  use = "pairwise.complete"
)
```

Not robust to outliers

 - Non-parametric correlation (Spearman Rank, Kendall Tau)
 - Bootstrap estimation of confidence interval


## Least squares regression

Describe relationship between response _y_ and predictor _x_
$$y = β_0 + β_1x$$

```{r lin-mods, echo = FALSE, fig.width = 8, fig.height = 4, message = FALSE}
gentoo <- penguins |> filter(species == "Gentoo")
p1 <- ggplot(gentoo, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Body mass g", y = "Bill length mm", title = "Continuous")

p2 <- ggplot(penguins, aes(x = species, y = body_mass_g)) +
  ggbeeswarm::geom_quasirandom() +
  stat_summary(colour = "red", size = 1, fun.data = "mean_cl_normal", fun.args = list(na.rm = TRUE)) +
  labs(x = "Species", y = "Body mass g", title = "Categorical")

p1 + p2
```

::: notes
- Estimate model coefficients with uncertainty
- Test if terms in the model are significant
- Check the method's assumptions
- View the model diagnostics
- Make predictions with confidence intervals

:::

## We want the parameters $\beta$

$$y_i = \beta_0 + \beta_1x_i + \epsilon_i  $$

- $y$ = continuous response
- $x$ = continuous or categorical predictor
- $i = 1, ..., n$ observations - $(xi, yi)$ observation pairs
- $ε_i$ = residual at $i$
- $β_0$ = mean, intercept
- $β_1$ = effect, slope

Residuals are actual - predicted



## Criteria

- $\epsilon \sim N(0, \sigma)$ 
- $y \sim N(\mu, \sigma)$

Use when

- Response variable is continuous
- Predictor variable(s) are continuous or categorical
- Observations are independent

+ Other assumptions are met

- Direction of causality is clear
- Want to make predictions
- Want effect size as slope or differences between groups


## Correlation or linear model?

Studying foraminifera test composition and temperature

1) Experiment with forams in tanks at different temperatures.

- foram Mg concentration, tank temperature
- foram Mg concentration, foram Ba concentration 

2) Observational study of forams collected from the ocean.

- foram Mg concentration, Ocean temperature
- Ocean temperature, ocean Mg concentration
- foram Mg concentration, foram species

## Which distribution matters?

{{< include includes/_x-distributions.qmd >}}



## Estimating $\beta$


Choose $\beta$ that minimise the sum of squares of residuals

$$\sum_{i = 1}^{n}\epsilon_i^2 =   \sum_{i = 1}^{n}(y_i - (\beta_0 + \beta_1x_i))^2$$

---

{{< include includes/_regression-line-ss.qmd >}}

## Calculating $\beta$


$$\beta_1 = \frac{s_{xy}}{s_x^2}$$
 Covariance of xy / variance of x
 
$$\beta_0 = mean(y) - \beta_1 mean(x)$$


## Fitting a least-squares model in R

```{r}
gentoo <- penguins |> filter(species == "Gentoo")

mod <- lm(bill_length_mm ~ body_mass_g, data = gentoo)
mod
```

## Summary()

```{r}
summary(mod)
```


## Variance partitioning

```{r, echo = FALSE}
ss_plot <- function(data, x, y) {
  data <- data |> drop_na(.data[[x]], .data[[y]])
  mod <- lm(as.formula(glue("{y} ~ {x}")), data = data)
  res <- augment(mod) |>
    mutate(
      total = mean(.data[[y]]),
    )
  reg <- tibble(
    what = c("Residual", "Regression", "Total", "Regression"),
    intercept = c(rep(coef(mod)[1], 2), rep(mean(data[[y]]), 2)),
    slope = c(rep(coef(mod)[2], 2), 0, 0)
  ) |>
    mutate(
      what = factor(what, levels = c("Regression", "Residual", "Total")),
      what = fct_rev(what)
    )


  res2 <- bind_rows(
    Total = res |>
      mutate(y = .data[[y]], yend = total),
    Residual = res |>
      mutate(y = .data[[y]], yend = .fitted),
    Regression = res |>
      mutate(y = .fitted, yend = total),
    .id = "what"
  ) |>
    mutate(
      what = factor(what, levels = c("Regression", "Residual", "Total")),
      what = fct_rev(what)
    )


  res2 |>
    ggplot(aes(x = .data[[x]], y = .data[[y]])) +
    geom_point() +
    geom_segment(
      aes(
        x = .data[[x]],
        xend = .data[[x]],
        y = y,
        yend = yend,
        colour = what
      ),
      show.legend = FALSE
    ) +
    geom_abline(data = reg, aes(intercept = intercept, slope = slope)) +
    facet_wrap(~what, ncol = 1)
}

sum_sq_plot <- penguins |>
  filter(species == "Gentoo") |>
  drop_na(bill_length_mm, body_mass_g) |>
  ss_plot(x = "body_mass_g", y = "bill_length_mm") +
  labs(x = "Body mass g", y = "Bill length mm")
```

:::: {.columns}

::: {.column width="60%"}

**Total sum of squares** [$SS_{total}$]{style="color: red;"}

Squared differences of observation from mean

**Residual sum of squares** [$SS_{residual}$]{style="color: green;"}

Squared differences of observation from regression line

**Regression sum of squares** [$SS_{regression}$]{style="color: blue;"}

Squared differences of regression line from mean
:::

::: {.column width="40%"}
```{r ss-plot, echo=FALSE, fig.height = 7, fig.width=5}
sum_sq_plot
```
:::

::::


## $R^2$

Coefficient of determination 

Coefficient of multiple correlation

  $$R^2 = 1 - \frac{\color{green}{SS_{residual}}}{\color{red}{SS_{total}}}$$
 
 - 0 - 1
 - $R^2$ = 0.5  -- 50% of variation in data explained
 - Always increases with more predictors
 
## Adjusted $R^2$
 
Corrects for number of parameters 

$$
R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}
$$
$R^2$ = R squared  
$n$ = number of observations  
$p$ = number of parameters  

Only increases if useful predictors added  
Can be negative
 
## Using `Anova`

$$F = \frac{\color{blue}{SS_{regression}}/df_{regression}}{\color{green}{SS_{residual}}/df_{residual}}$$

```{r}
car::Anova(mod)
```


## The F distribution

{{< include includes/_f_test.qmd >}}


## Categorical predictors

:::: {.columns}

::: {.column width="60%"}
Does penguin body mass depend on species?

Predictor = species (categorical)

Response = body mass (continuous)

Hypotheses 

$$H_0: \mu_{Adelie} = \mu_{Chinstrap} = \mu_{Gentoo}$$
$H_A$ At least two of the means differ

:::

::: {.column width="40%"}
```{r species-plot, echo=FALSE}
ggplot(penguins, aes(x = species, y = body_mass_g, colour = species, fill = after_scale(colorspace::lighten(color, 0.9)))) +
  geom_violin(show.legend = FALSE) +
  ggbeeswarm::geom_quasirandom(show.legend = FALSE) +
  scale_fill_brewer(palette = "Set1") +
  labs(x = "Species", y = "Body mass g")
```
:::

::::



## Fitting the model

```{r}
mod2 <- lm(body_mass_g ~ species, data = penguins)
mod2
```


## `Anova` for categorical variables

```{r}
car::Anova(mod2)
```

At least two groups differ


## `summary` for categorical variables

```{r}
summary(mod2)
```

---

Summary shows difference between 

- Adelie and Chinstrap
- Adelie and Gentoo

Not between 

 - Chinstrap and Gentoo


## Forcing the reference level

Very useful when you have a control

```{r}
penguins2 <- penguins |>
  mutate(species = factor(species, levels = c("Gentoo", "Adelie", "Chinstrap")))

mod3 <- lm(body_mass_g ~ species, data = penguins2)
```


```{r}
broom::tidy(mod3)
```

## Multiple comparisons

- Don't change the order of the levels of the predictor variable to 
compare all groups against each other
- use a post-hoc multiple comparisons test

```{r, results = 'hide'}
library(multcomp) # need to be using conflicted package or disaster

mc <- glht(mod2, linfct = mcp(species = "Tukey"))
summary(mc)
```

---

```{r, echo=FALSE}
summary(mc)
```

---

```{r}
#| label: multicomp-figure
ggplot(confint(mc), aes(x = lhs, y = estimate, ymin = lwr, ymax = upr)) +
  geom_pointrange() +
  labs(x = NULL)
```


## Assumptions of Least Squares

1. The relationship between the response and the predictors is ~linear.
2. The residuals have a mean of zero.
3. The residuals have constant variance (not heteroscedastic).
4. The residuals are independent (uncorrelated).
5. The residuals are normally distributed.

Violation of assumptions cannot be detected using the _t_ or _F_ statistics or R<sup>2</sup>

## Diagnostic plots

- `plot()` base R diagnostic plots
- `autoplot()` from `ggfortify` - same plots but prettier
- `check_model()` from `performance` - more diagnostics
- `DHARMa` package - very powerful diagnostics

## `performance`


```{r}
#| label: fig-check-model  

library(performance)
check_model(mod)  
```

## Posterior predictive checks


:::: {.columns}

::: {.column width="50%"}
Check for systematic discrepancies between real and simulated data

- green - density of observed response
- blue - density of simulated response

Want green line to resemble blue lines
:::

::: {.column width="50%"}

```{r}
#| label: fig-posterior-predictive
#| echo: false
# return a list of single plots
diagnostic_plots <- plot(check_model(mod, panel = FALSE))
# posterior predicive checks
diagnostic_plots$PP_CHECK
```

:::

::::


## Residual vs fitted

:::: {.columns}

::: {.column width="50%"}
Check for

- Outliers
- Variations in the mean residual

Want flat line
:::

::: {.column width="50%"}
```{r}
#| label: fig-residual-fitted
#| echo: false
diagnostic_plots$NCV
```
:::

::::



## Quantile-quantile plot

:::: {.columns}

::: {.column width="50%"}
QQ plots compare two
samples to determine if they are from the same distribution.


Check for

- Non-normal distribution of the residuals

Points will lie on straight line if normally distributed
:::

::: {.column width="50%"}
```{r}
#| label: fig-qq
#| echo: false
# normally distributed residuals
diagnostic_plots$QQ
```
:::

::::

## Scale-location plot

:::: {.columns}

::: {.column width="50%"}
Square root of the absolute standardised residuals 

Check for

- Unequal variance
= Heteroscedasticity

Want flat line
:::

::: {.column width="50%"}
```{r}
#| label: fig-scale-location
#| echo: false
# homoscedasticiy - homogeneity of variance
diagnostic_plots$HOMOGENEITY
```
:::

::::




## Residuals vs leverage

:::: {.columns}

::: {.column width="50%"}
Plot of standardised residuals against leverage, with contours of Cooks distance

Observations with extreme leverage should be checked
:::

::: {.column width="50%"}
```{r}
#| label: fig-residual-leverage
#| echo: false
diagnostic_plots$OUTLIERS
```
:::

::::




## Predictions

:::: {.columns}

::: {.column width="50%"}
$$y = \beta_0+\beta_1x$$
:::

::: {.column width="50%"}
```{r pred-plot, echo = FALSE}
x <- 5000
ggplot(gentoo, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point() +
  geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2], colour = scales::muted("red")) +
  geom_vline(xintercept = x, linetype = "dashed") +
  geom_hline(yintercept = coef(mod)[1] + coef(mod)[2] * x, linetype = "dashed") +
  labs(x = "Body mass g", y = "Bill length mm")
```
:::

::::






## Predict

```{r}
predict(object = mod)
```

## Predict with new data

```{r}
nd <- tibble(body_mass_g = c(5000, 7000)) # <1>
predict(mod, newdata = nd) # <2>
```

1. Make tibble or data.frame with new data. Must include all predictors.
2. Use `predict()` with the model and new data.

## Predictions with standard errors

Uncertainty of the mean

```{r}
predict(mod, newdata = nd, se.fit = TRUE)
```

## Predictions with confidence interval

```{r}
predict(mod, newdata = nd, interval = "confidence", level = 0.95)
```

---

Often easier to use `broom::augment()`

```{r augment, fig.height = 5}
augment(mod, interval = "confidence") |>
  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point() +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = .3) +
  geom_line(aes(y = .fitted))
```


## Predictions interval

Where will a new observation probably be


```{r prediction-interval, fig.height = 5}
augment(mod, interval = "prediction") |>
  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point() +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), alpha = 0.3) +
  geom_line(aes(y = .fitted))
```
